{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad350ac0",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833688bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据准备脚本\n",
    "功能：\n",
    "1. 从YOLO格式数据集读取并划分训练集和验证集\n",
    "2. 为目标检测准备数据（所有类别统一为\"fly\"）\n",
    "3. 为分类准备裁剪后的图像数据集（Long/Short/Ambiguous）\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, yolo_data_root, output_root, train_ratio=0.8, exclude_ambiguous=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            yolo_data_root: YOLO格式数据根目录（包含images/和labels/子目录）\n",
    "            output_root: 输出目录\n",
    "            train_ratio: 训练集比例\n",
    "            exclude_ambiguous: 是否排除Ambiguous类别\n",
    "        \"\"\"\n",
    "        self.yolo_data_root = Path(yolo_data_root)\n",
    "        self.images_dir = self.yolo_data_root / 'images'\n",
    "        self.labels_dir = self.yolo_data_root / 'labels'\n",
    "        self.classes_file = self.yolo_data_root / 'classes.txt'\n",
    "        self.output_root = Path(output_root)\n",
    "        self.train_ratio = train_ratio\n",
    "        self.exclude_ambiguous = exclude_ambiguous\n",
    "        \n",
    "        # 读取类别\n",
    "        with open(self.classes_file, 'r', encoding='utf-8') as f:\n",
    "            self.classes = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        print(f\"类别: {self.classes}\")\n",
    "        \n",
    "        # 获取所有图像文件\n",
    "        self.image_files = sorted(list(self.images_dir.glob('*.jpg')) + list(self.images_dir.glob('*.png')))\n",
    "        \n",
    "        # 过滤出有对应标注文件的图像\n",
    "        self.valid_images = []\n",
    "        for img_path in self.image_files:\n",
    "            label_path = self.labels_dir / (img_path.stem + '.txt')\n",
    "            if label_path.exists():\n",
    "                self.valid_images.append(img_path)\n",
    "        \n",
    "        print(f\"加载数据集: {len(self.valid_images)} 张有效图片（共 {len(self.image_files)} 张）\")\n",
    "        \n",
    "    def parse_yolo_label(self, label_path, img_width, img_height):\n",
    "        \"\"\"解析YOLO格式标注文件\n",
    "        \n",
    "        Args:\n",
    "            label_path: 标注文件路径\n",
    "            img_width: 图像宽度\n",
    "            img_height: 图像高度\n",
    "            \n",
    "        Returns:\n",
    "            boxes: [(class_id, x1, y1, x2, y2), ...]\n",
    "        \"\"\"\n",
    "        boxes = []\n",
    "        \n",
    "        if not label_path.exists():\n",
    "            return boxes\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                \n",
    "                class_id = int(parts[0])\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "                \n",
    "                # 转换为绝对坐标\n",
    "                x1 = (x_center - width / 2) * img_width\n",
    "                y1 = (y_center - height / 2) * img_height\n",
    "                x2 = (x_center + width / 2) * img_width\n",
    "                y2 = (y_center + height / 2) * img_height\n",
    "                \n",
    "                boxes.append((class_id, x1, y1, x2, y2))\n",
    "        \n",
    "        return boxes\n",
    "        \n",
    "    def prepare_detection_data(self):\n",
    "        \"\"\"准备目标检测数据（YOLO格式，所有类别统一为fly）\"\"\"\n",
    "        print(\"\\n=== 准备目标检测数据 ===\")\n",
    "        \n",
    "        # 创建输出目录\n",
    "        detection_root = self.output_root / \"detection\"\n",
    "        for split in ['train', 'val']:\n",
    "            (detection_root / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "            (detection_root / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 划分训练集和验证集\n",
    "        images = self.valid_images.copy()\n",
    "        random.shuffle(images)\n",
    "        train_size = int(len(images) * self.train_ratio)\n",
    "        train_images = images[:train_size]\n",
    "        val_images = images[train_size:]\n",
    "        \n",
    "        print(f\"训练集: {len(train_images)} 张, 验证集: {len(val_images)} 张\")\n",
    "        \n",
    "        # 处理训练集和验证集\n",
    "        for split, img_list in [('train', train_images), ('val', val_images)]:\n",
    "            print(f\"\\n处理{split}集...\")\n",
    "            for img_path in tqdm(img_list):\n",
    "                # 复制图片\n",
    "                output_img_path = detection_root / split / 'images' / img_path.name\n",
    "                shutil.copy(img_path, output_img_path)\n",
    "                \n",
    "                # 读取并转换标注（所有类别统一为0）\n",
    "                label_path = self.labels_dir / (img_path.stem + '.txt')\n",
    "                output_label_path = detection_root / split / 'labels' / (img_path.stem + '.txt')\n",
    "                \n",
    "                with open(label_path, 'r') as f_in, open(output_label_path, 'w') as f_out:\n",
    "                    for line in f_in:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            # 将所有类别改为0（统一为fly）\n",
    "                            f_out.write(f\"0 {' '.join(parts[1:])}\\n\")\n",
    "        \n",
    "        # 生成data.yaml配置文件\n",
    "        yaml_content = f\"\"\"path: {detection_root.absolute()}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: 1\n",
    "names: ['fly']\n",
    "\"\"\"\n",
    "        with open(detection_root / 'data.yaml', 'w') as f:\n",
    "            f.write(yaml_content)\n",
    "        \n",
    "        print(f\"\\n✅ 目标检测数据准备完成！保存在: {detection_root}\")\n",
    "        return detection_root / 'data.yaml'\n",
    "        \n",
    "    def prepare_classification_data(self):\n",
    "        \"\"\"准备分类数据（裁剪后的图像，按类别分类）\"\"\"\n",
    "        print(\"\\n=== 准备分类数据 ===\")\n",
    "        \n",
    "        # 创建输出目录\n",
    "        classification_root = self.output_root / \"classification\"\n",
    "        \n",
    "        # 确定要处理的类别\n",
    "        if self.exclude_ambiguous:\n",
    "            target_classes = [c for c in self.classes if c != 'Ambiguous']\n",
    "        else:\n",
    "            target_classes = self.classes\n",
    "        \n",
    "        print(f\"处理类别: {target_classes}\")\n",
    "        \n",
    "        for split in ['train', 'val']:\n",
    "            for category in target_classes:\n",
    "                (classification_root / split / category).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 划分训练集和验证集（使用相同的划分）\n",
    "        images = self.valid_images.copy()\n",
    "        random.shuffle(images)\n",
    "        train_size = int(len(images) * self.train_ratio)\n",
    "        train_images = set(images[:train_size])\n",
    "        \n",
    "        # 处理所有图像\n",
    "        print(\"裁剪并保存图像...\")\n",
    "        crop_counts = {'train': {}, 'val': {}}\n",
    "        \n",
    "        for img_path in tqdm(self.valid_images):\n",
    "            # 确定是训练集还是验证集\n",
    "            split = 'train' if img_path in train_images else 'val'\n",
    "            \n",
    "            # 读取图像\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"无法打开图像 {img_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # 读取标注\n",
    "            label_path = self.labels_dir / (img_path.stem + '.txt')\n",
    "            boxes = self.parse_yolo_label(label_path, img.width, img.height)\n",
    "            \n",
    "            # 裁剪每个标注框\n",
    "            for idx, (class_id, x1, y1, x2, y2) in enumerate(boxes):\n",
    "                category_name = self.classes[class_id]\n",
    "                \n",
    "                # 如果排除Ambiguous类别\n",
    "                if self.exclude_ambiguous and category_name == 'Ambiguous':\n",
    "                    continue\n",
    "                \n",
    "                # 确保边界在图像内\n",
    "                x1 = max(0, int(x1))\n",
    "                y1 = max(0, int(y1))\n",
    "                x2 = min(img.width, int(x2))\n",
    "                y2 = min(img.height, int(y2))\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "                \n",
    "                # 裁剪\n",
    "                cropped = img.crop((x1, y1, x2, y2))\n",
    "                \n",
    "                # 保存\n",
    "                if category_name not in crop_counts[split]:\n",
    "                    crop_counts[split][category_name] = 0\n",
    "                crop_counts[split][category_name] += 1\n",
    "                \n",
    "                crop_filename = f\"{img_path.stem}_{idx}_{category_name}.jpg\"\n",
    "                output_path = classification_root / split / category_name / crop_filename\n",
    "                cropped.save(output_path, quality=95)\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"\\n✅ 分类数据准备完成！保存在: {classification_root}\")\n",
    "        print(\"\\n数据集统计:\")\n",
    "        for split in ['train', 'val']:\n",
    "            print(f\"\\n{split}集:\")\n",
    "            total = sum(crop_counts[split].values())\n",
    "            for category, count in sorted(crop_counts[split].items()):\n",
    "                percentage = 100 * count / total if total > 0 else 0\n",
    "                print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        return classification_root\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设置随机种子\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 配置路径\n",
    "    yolo_data_root = \"data_yolo\"  # YOLO格式数据目录\n",
    "    output_root = \"processed_data\"\n",
    "    \n",
    "    # 创建数据准备对象\n",
    "    data_prep = DataPreparation(\n",
    "        yolo_data_root=yolo_data_root,\n",
    "        output_root=output_root,\n",
    "        train_ratio=0.8,\n",
    "        exclude_ambiguous=True  # 排除Ambiguous类别\n",
    "    )\n",
    "    \n",
    "    # 准备目标检测数据\n",
    "    yaml_path = data_prep.prepare_detection_data()\n",
    "    \n",
    "    # 准备分类数据\n",
    "    classification_root = data_prep.prepare_classification_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ 数据准备完成！\")\n",
    "    print(f\"检测数据配置: {yaml_path}\")\n",
    "    print(f\"分类数据目录: {classification_root}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acf0ce",
   "metadata": {},
   "source": [
    "## train_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a5078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "果蝇目标检测训练脚本（YOLOv8）\n",
    "使用YOLOv8进行果蝇检测（不区分类别）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "\n",
    "class FlyDetectionTrainer:\n",
    "    def __init__(self, data_yaml, model_name='yolov8n.pt', epochs=100, img_size=640, batch_size=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_yaml: 数据配置文件路径\n",
    "            model_name: 预训练模型名称\n",
    "            epochs: 训练轮数\n",
    "            img_size: 图像尺寸\n",
    "            batch_size: 批次大小\n",
    "        \"\"\"\n",
    "        self.data_yaml = data_yaml\n",
    "        self.model_name = model_name\n",
    "        self.epochs = epochs\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train(self, project='runs/detect', name='fly_detector'):\n",
    "        \"\"\"训练目标检测模型\"\"\"\n",
    "        print(\"=== 开始训练果蝇检测模型 ===\")\n",
    "        print(f\"模型: {self.model_name}\")\n",
    "        print(f\"数据配置: {self.data_yaml}\")\n",
    "        print(f\"训练轮数: {self.epochs}\")\n",
    "        print(f\"图像尺寸: {self.img_size}\")\n",
    "        print(f\"批次大小: {self.batch_size}\")\n",
    "        print(f\"设备: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "        \n",
    "        # 加载预训练模型\n",
    "        model = YOLO(self.model_name)\n",
    "        \n",
    "        # 训练参数\n",
    "        results = model.train(\n",
    "            data=self.data_yaml,\n",
    "            epochs=self.epochs,\n",
    "            imgsz=self.img_size,\n",
    "            batch=self.batch_size,\n",
    "            project=project,\n",
    "            name=name,\n",
    "            patience=20,  # 早停耐心值\n",
    "            save=True,\n",
    "            save_period=10,  # 每10个epoch保存一次\n",
    "            # 数据增强\n",
    "            hsv_h=0.015,\n",
    "            hsv_s=0.7,\n",
    "            hsv_v=0.4,\n",
    "            degrees=10.0,\n",
    "            translate=0.1,\n",
    "            scale=0.5,\n",
    "            shear=0.0,\n",
    "            perspective=0.0,\n",
    "            flipud=0.5,\n",
    "            fliplr=0.5,\n",
    "            mosaic=1.0,\n",
    "            mixup=0.1,\n",
    "            # 优化器\n",
    "            optimizer='AdamW',\n",
    "            lr0=0.01,\n",
    "            lrf=0.01,\n",
    "            momentum=0.937,\n",
    "            weight_decay=0.0005,\n",
    "            warmup_epochs=3,\n",
    "            warmup_momentum=0.8,\n",
    "            warmup_bias_lr=0.1,\n",
    "            # 其他\n",
    "            cos_lr=True,\n",
    "            label_smoothing=0.0,\n",
    "            box=7.5,\n",
    "            cls=0.5,\n",
    "            dfl=1.5,\n",
    "            plots=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✅ 训练完成！\")\n",
    "        print(f\"最佳模型保存在: {Path(project) / name / 'weights' / 'best.pt'}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate(self, model_path, project='runs/detect', name='val'):\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        print(\"\\n=== 验证模型 ===\")\n",
    "        model = YOLO(model_path)\n",
    "        \n",
    "        results = model.val(\n",
    "            data=self.data_yaml,\n",
    "            imgsz=self.img_size,\n",
    "            batch=self.batch_size,\n",
    "            project=project,\n",
    "            name=name,\n",
    "            plots=True,\n",
    "        )\n",
    "        \n",
    "        print(\"\\n验证结果:\")\n",
    "        print(f\"mAP50: {results.box.map50:.4f}\")\n",
    "        print(f\"mAP50-95: {results.box.map:.4f}\")\n",
    "        print(f\"Precision: {results.box.mp:.4f}\")\n",
    "        print(f\"Recall: {results.box.mr:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 配置参数\n",
    "    data_yaml = \"processed_data/detection/data.yaml\"\n",
    "    \n",
    "    # 创建训练器\n",
    "    trainer = FlyDetectionTrainer(\n",
    "        data_yaml=data_yaml,\n",
    "        model_name='yolov8n.pt',  # 可选: yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt\n",
    "        epochs=100,\n",
    "        img_size=640,\n",
    "        batch_size=16,\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    results = trainer.train(project='runs/detect', name='fly_detector')\n",
    "    \n",
    "    # 验证最佳模型\n",
    "    best_model_path = 'runs/detect/fly_detector/weights/best.pt'\n",
    "    if Path(best_model_path).exists():\n",
    "        trainer.validate(best_model_path, project='runs/detect', name='val_best')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f880d",
   "metadata": {},
   "source": [
    "## train_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b3662",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "果蝇翅膀分类训练脚本\n",
    "使用ResNet进行长翅/短翅分类\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class FlyClassificationDataset(Dataset):\n",
    "    \"\"\"果蝇分类数据集\"\"\"\n",
    "    def __init__(self, data_root, split='train', transform=None, exclude_ambiguous=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: 数据根目录\n",
    "            split: 'train' 或 'val'\n",
    "            transform: 数据转换\n",
    "            exclude_ambiguous: 是否排除Ambiguous类别\n",
    "        \"\"\"\n",
    "        self.data_root = Path(data_root) / split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 类别映射\n",
    "        if exclude_ambiguous:\n",
    "            self.class_to_idx = {'Long': 0, 'Short': 1}\n",
    "            self.classes = ['Long', 'Short']\n",
    "        else:\n",
    "            self.class_to_idx = {'Ambiguous': 0, 'Long': 1, 'Short': 2}\n",
    "            self.classes = ['Ambiguous', 'Long', 'Short']\n",
    "        \n",
    "        # 加载图像路径\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.data_root / class_name\n",
    "            if not class_dir.exists():\n",
    "                continue\n",
    "            for img_path in class_dir.glob('*.jpg'):\n",
    "                self.samples.append((str(img_path), self.class_to_idx[class_name]))\n",
    "        \n",
    "        print(f\"{split}集: {len(self.samples)} 个样本\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "class FlyClassifier:\n",
    "    def __init__(self, num_classes=2, model_name='resnet50', pretrained=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: 类别数量\n",
    "            model_name: 模型名称 ('resnet18', 'resnet50', 'efficientnet_b0', etc.)\n",
    "            pretrained: 是否使用预训练权重\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 创建模型\n",
    "        self.model = self._create_model(pretrained)\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        print(f\"模型: {model_name}, 设备: {self.device}, 类别数: {num_classes}\")\n",
    "        \n",
    "    def _create_model(self, pretrained):\n",
    "        \"\"\"创建模型\"\"\"\n",
    "        if self.model_name == 'resnet18':\n",
    "            model = models.resnet18(pretrained=pretrained)\n",
    "            model.fc = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        elif self.model_name == 'resnet50':\n",
    "            model = models.resnet50(pretrained=pretrained)\n",
    "            model.fc = nn.Linear(model.fc.in_features, self.num_classes)\n",
    "        elif self.model_name == 'efficientnet_b0':\n",
    "            model = models.efficientnet_b0(pretrained=pretrained)\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, self.num_classes)\n",
    "        elif self.model_name == 'mobilenet_v2':\n",
    "            model = models.mobilenet_v2(pretrained=pretrained)\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, self.num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的模型: {self.model_name}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=50, lr=0.001, save_dir='runs/classification'):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        \n",
    "        print(\"\\n=== 开始训练 ===\")\n",
    "        for epoch in range(epochs):\n",
    "            # 训练阶段\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                                 'acc': f'{100.*train_correct/train_total:.2f}%'})\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_loss, val_acc = self.validate(val_loader, criterion)\n",
    "            \n",
    "            # 更新学习率\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 记录历史\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                    'model_name': self.model_name,\n",
    "                    'num_classes': self.num_classes,\n",
    "                }, save_dir / 'best_model.pth')\n",
    "                print(f\"✅ 保存最佳模型 (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # 保存训练历史\n",
    "        with open(save_dir / 'history.json', 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        \n",
    "        # 绘制训练曲线\n",
    "        self.plot_history(history, save_dir)\n",
    "        \n",
    "        print(f\"\\n✅ 训练完成！最佳验证准确率: {best_acc:.2f}%\")\n",
    "        print(f\"模型保存在: {save_dir / 'best_model.pth'}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def validate(self, val_loader, criterion=None):\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if criterion is None:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def evaluate(self, test_loader, class_names, save_dir='runs/classification'):\n",
    "        \"\"\"详细评估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"\\n=== 评估模型 ===\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc=\"评估中\"):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # 分类报告\n",
    "        print(\"\\n分类报告:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "        \n",
    "        # 混淆矩阵\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_dir = Path(save_dir)\n",
    "        plt.savefig(save_dir / 'confusion_matrix.png')\n",
    "        print(f\"\\n混淆矩阵保存在: {save_dir / 'confusion_matrix.png'}\")\n",
    "        \n",
    "        return all_preds, all_labels\n",
    "    \n",
    "    def plot_history(self, history, save_dir):\n",
    "        \"\"\"绘制训练历史\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(history['train_loss'], label='Train Loss')\n",
    "        ax1.plot(history['val_loss'], label='Val Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy\n",
    "        ax2.plot(history['train_acc'], label='Train Acc')\n",
    "        ax2.plot(history['val_acc'], label='Val Acc')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(save_dir) / 'training_history.png')\n",
    "        print(f\"训练曲线保存在: {Path(save_dir) / 'training_history.png'}\")\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"加载模型: {model_path}\")\n",
    "        print(f\"最佳准确率: {checkpoint['best_acc']:.2f}%\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 配置参数\n",
    "    data_root = \"processed_data/classification\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    img_size = 224\n",
    "    \n",
    "    # 数据转换\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = FlyClassificationDataset(data_root, 'train', train_transform, exclude_ambiguous=True)\n",
    "    val_dataset = FlyClassificationDataset(data_root, 'val', val_transform, exclude_ambiguous=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # 创建分类器\n",
    "    classifier = FlyClassifier(num_classes=2, model_name='resnet50', pretrained=True)\n",
    "    \n",
    "    # 训练\n",
    "    history = classifier.train(train_loader, val_loader, epochs=num_epochs, lr=learning_rate)\n",
    "    \n",
    "    # 评估\n",
    "    classifier.evaluate(val_loader, class_names=['Long', 'Short'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bdfe6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd7212",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "端到端推理脚本\n",
    "完整的检测+分类流程\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "class FlyDetectionClassificationPipeline:\n",
    "    \"\"\"果蝇检测和分类端到端流程\"\"\"\n",
    "    \n",
    "    def __init__(self, detection_model_path, classification_model_path, \n",
    "                 classification_model_name='resnet50', num_classes=2, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            detection_model_path: 检测模型路径\n",
    "            classification_model_path: 分类模型路径\n",
    "            classification_model_name: 分类模型名称\n",
    "            num_classes: 分类类别数\n",
    "            conf_threshold: 检测置信度阈值\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.conf_threshold = conf_threshold\n",
    "        \n",
    "        # 加载检测模型\n",
    "        print(\"加载检测模型...\")\n",
    "        self.detection_model = YOLO(detection_model_path)\n",
    "        \n",
    "        # 加载分类模型\n",
    "        print(\"加载分类模型...\")\n",
    "        self.classification_model = self._load_classification_model(\n",
    "            classification_model_path, classification_model_name, num_classes\n",
    "        )\n",
    "        \n",
    "        # 分类类别\n",
    "        self.class_names = ['Long', 'Short']\n",
    "        \n",
    "        # 分类图像转换\n",
    "        self.classification_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"✅ 模型加载完成！设备: {self.device}\")\n",
    "    \n",
    "    def _load_classification_model(self, model_path, model_name, num_classes):\n",
    "        \"\"\"加载分类模型\"\"\"\n",
    "        # 创建模型架构\n",
    "        if model_name == 'resnet18':\n",
    "            model = models.resnet18()\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        elif model_name == 'resnet50':\n",
    "            model = models.resnet50()\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        elif model_name == 'efficientnet_b0':\n",
    "            model = models.efficientnet_b0()\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        elif model_name == 'mobilenet_v2':\n",
    "            model = models.mobilenet_v2()\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的模型: {model_name}\")\n",
    "        \n",
    "        # 加载权重\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def detect_flies(self, image):\n",
    "        \"\"\"检测图像中的果蝇\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image 或 numpy array\n",
    "            \n",
    "        Returns:\n",
    "            boxes: 检测框 [[x1, y1, x2, y2, conf], ...]\n",
    "        \"\"\"\n",
    "        results = self.detection_model(image, conf=self.conf_threshold, verbose=False)\n",
    "        \n",
    "        boxes = []\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                conf = box.conf[0].cpu().numpy()\n",
    "                boxes.append([x1, y1, x2, y2, conf])\n",
    "        \n",
    "        return np.array(boxes) if boxes else np.array([]).reshape(0, 5)\n",
    "    \n",
    "    def classify_fly(self, image, box):\n",
    "        \"\"\"对检测到的果蝇进行分类\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            box: [x1, y1, x2, y2, conf]\n",
    "            \n",
    "        Returns:\n",
    "            class_name: 类别名称\n",
    "            confidence: 分类置信度\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = map(int, box[:4])\n",
    "        \n",
    "        # 裁剪果蝇区域\n",
    "        cropped = image.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        # 转换图像\n",
    "        img_tensor = self.classification_transform(cropped).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # 分类\n",
    "        with torch.no_grad():\n",
    "            outputs = self.classification_model(img_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        class_name = self.class_names[predicted.item()]\n",
    "        confidence = confidence.item()\n",
    "        \n",
    "        return class_name, confidence\n",
    "    \n",
    "    def process_image(self, image_path, save_path=None, save_crops=False, crops_dir=None):\n",
    "        \"\"\"处理单张图像\n",
    "        \n",
    "        Args:\n",
    "            image_path: 图像路径\n",
    "            save_path: 结果保存路径\n",
    "            save_crops: 是否保存裁剪的果蝇图像\n",
    "            crops_dir: 裁剪图像保存目录\n",
    "            \n",
    "        Returns:\n",
    "            results: [{'box': [x1, y1, x2, y2], 'det_conf': conf, \n",
    "                      'class': class_name, 'cls_conf': conf}, ...]\n",
    "        \"\"\"\n",
    "        # 读取图像\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # 检测果蝇\n",
    "        boxes = self.detect_flies(image)\n",
    "        \n",
    "        results = []\n",
    "        for i, box in enumerate(boxes):\n",
    "            # 分类\n",
    "            class_name, cls_conf = self.classify_fly(image, box)\n",
    "            \n",
    "            result = {\n",
    "                'box': box[:4].tolist(),\n",
    "                'det_conf': float(box[4]),\n",
    "                'class': class_name,\n",
    "                'cls_conf': float(cls_conf)\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # 保存裁剪图像\n",
    "            if save_crops and crops_dir:\n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                cropped = image.crop((x1, y1, x2, y2))\n",
    "                crop_path = Path(crops_dir) / class_name / f\"{Path(image_path).stem}_{i}.jpg\"\n",
    "                crop_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                cropped.save(crop_path)\n",
    "        \n",
    "        # 可视化并保存\n",
    "        if save_path:\n",
    "            self.visualize_results(image_path, results, save_path)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_batch(self, image_dir, output_dir, save_crops=False):\n",
    "        \"\"\"批量处理图像\n",
    "        \n",
    "        Args:\n",
    "            image_dir: 图像目录\n",
    "            output_dir: 输出目录\n",
    "            save_crops: 是否保存裁剪图像\n",
    "        \"\"\"\n",
    "        image_dir = Path(image_dir)\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        vis_dir = output_dir / 'visualizations'\n",
    "        vis_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        crops_dir = output_dir / 'crops' if save_crops else None\n",
    "        if crops_dir:\n",
    "            for class_name in self.class_names:\n",
    "                (crops_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 获取所有图像\n",
    "        image_paths = list(image_dir.glob('*.jpg')) + list(image_dir.glob('*.png'))\n",
    "        \n",
    "        all_results = {}\n",
    "        stats = {'Long': 0, 'Short': 0}\n",
    "        \n",
    "        print(f\"\\n处理 {len(image_paths)} 张图像...\")\n",
    "        for img_path in tqdm(image_paths):\n",
    "            save_path = vis_dir / img_path.name\n",
    "            results = self.process_image(img_path, save_path, save_crops, crops_dir)\n",
    "            \n",
    "            all_results[img_path.name] = results\n",
    "            \n",
    "            # 统计\n",
    "            for result in results:\n",
    "                stats[result['class']] += 1\n",
    "        \n",
    "        # 保存结果\n",
    "        with open(output_dir / 'results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"\\n✅ 处理完成！\")\n",
    "        print(f\"结果保存在: {output_dir}\")\n",
    "        print(f\"\\n统计信息:\")\n",
    "        total = sum(stats.values())\n",
    "        for class_name, count in stats.items():\n",
    "            percentage = 100 * count / total if total > 0 else 0\n",
    "            print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 保存统计\n",
    "        with open(output_dir / 'statistics.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return all_results, stats\n",
    "    \n",
    "    def visualize_results(self, image_path, results, save_path):\n",
    "        \"\"\"可视化检测和分类结果\"\"\"\n",
    "        # 读取图像\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 绘制\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        colors = {'Long': 'green', 'Short': 'red'}\n",
    "        \n",
    "        for result in results:\n",
    "            x1, y1, x2, y2 = result['box']\n",
    "            class_name = result['class']\n",
    "            det_conf = result['det_conf']\n",
    "            cls_conf = result['cls_conf']\n",
    "            \n",
    "            color = colors.get(class_name, 'blue')\n",
    "            \n",
    "            # 绘制检测框\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                     linewidth=2, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # 添加标签\n",
    "            label = f\"{class_name} {cls_conf:.2f}\"\n",
    "            ax.text(x1, y1-10, label, color='white', fontsize=10,\n",
    "                   bbox=dict(facecolor=color, alpha=0.7, edgecolor='none', pad=2))\n",
    "        \n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 配置路径\n",
    "    detection_model_path = \"runs/detect/fly_detector/weights/best.pt\"\n",
    "    classification_model_path = \"runs/classification/best_model.pth\"\n",
    "    \n",
    "    # 创建推理流程\n",
    "    pipeline = FlyDetectionClassificationPipeline(\n",
    "        detection_model_path=detection_model_path,\n",
    "        classification_model_path=classification_model_path,\n",
    "        classification_model_name='resnet50',\n",
    "        num_classes=2,\n",
    "        conf_threshold=0.25\n",
    "    )\n",
    "    \n",
    "    # 选择处理模式\n",
    "    import sys\n",
    "    if len(sys.argv) > 1:\n",
    "        mode = sys.argv[1]\n",
    "    else:\n",
    "        mode = 'batch'  # 默认批量处理\n",
    "    \n",
    "    if mode == 'single':\n",
    "        # 单张图像测试\n",
    "        if len(sys.argv) > 2:\n",
    "            image_path = sys.argv[2]\n",
    "        else:\n",
    "            image_path = \"data/flys/MVIMG_20251202_153157.jpg\"\n",
    "        \n",
    "        output_path = \"test_output.jpg\"\n",
    "        results = pipeline.process_image(image_path, save_path=output_path, \n",
    "                                        save_crops=True, crops_dir='test_crops')\n",
    "        \n",
    "        print(f\"\\n检测到 {len(results)} 只果蝇:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. {result['class']} (分类置信度: {result['cls_conf']:.3f}, \"\n",
    "                  f\"检测置信度: {result['det_conf']:.3f})\")\n",
    "    \n",
    "    else:\n",
    "        # 批量处理\n",
    "        if len(sys.argv) > 2:\n",
    "            image_dir = sys.argv[2]\n",
    "        else:\n",
    "            image_dir = \"processed_data/detection/val/images\"\n",
    "        \n",
    "        output_dir = \"inference_results\"\n",
    "        \n",
    "        all_results, stats = pipeline.process_batch(\n",
    "            image_dir=image_dir,\n",
    "            output_dir=output_dir,\n",
    "            save_crops=True\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
